import re
import hydra
import torch
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
from pshmodule.processing import processing as p


# device = "cuda" if torch.cuda.is_available() else "cpu"
device = torch.device("cuda")


@hydra.main(config_path="./", config_name="config")
def main(cfg):
    # tokenizer
    tokenizer = PreTrainedTokenizerFast.from_pretrained(cfg.PATH.save_dir)

    # model
    model = BartForConditionalGeneration.from_pretrained(cfg.PATH.save_dir)
    model.to(device)

    # convert_to_other_unicode
    nlp = p.Nlp()

    texts = [
        ["", "NT", "SF", "ë‹¹ì‹ ì€ ë§ì…ˆ ì²œì¬ì…ë‹ˆê¹Œ?\\\\ë§ì…ˆ ê²Œì„ ì°¸ì—¬ ì‹œ ğŸ’°31ë§Œ í¬ì¸íŠ¸ê°€ [TAG1]ë‹˜ ì£¼ë¨¸ë‹ˆë¡œ ì˜-ì˜¥"],
        [
            "ê°€ì„ë§ì´",
            "ST",
            "SF",
            "10ì›”ì´ë‹ˆê¹Œ 1OOOí¬ì¸íŠ¸â™¬\\\\ë¸Œëœë“œë³„ í• ì¸ ë°›ê³  1ì²œP ì ë¦½ê¹Œì§€ ë”!\\ì•Œëœ°í•˜ê²Œ ê°€ì„ ì˜· ì¤€ë¹„í•˜ë ¤ë©´ í„°ì¹˜â–¶",
        ],
        [
            "ì‹ í•™ê¸°",
            "NF",
            "NT",
            "ìš°ë¦¬ ì•„ì´ ì„±ì ë„ ìì‹ ê°ë„ í™ˆëŸ°âš¾\\\\ê·¸ ìœ ëª…í•œ ì´ˆë“±í•™ìŠµ 1ìœ„ [í™ˆëŸ°]\\ìƒˆí•™ê¸°ë¥¼ ë§ì•„ ë¬´ë£Œ ì²´í—˜ì„ ì œê³µí•´ìš”â£ï¸ ì—­ì‚¬ ì—°ëŒ€í‘œì™€ í¬ì¸íŠ¸ë„ ë“œë¦¬ë‹ˆê¹Œ ë¶€ë‹´ì—†ì´ ì‹ ì²­í•˜ì„¸ìš”ğŸ˜",
        ],
        [
            "ì–´ë²„ì´ë‚ ",
            "NT",
            "ST",
            "ì´ë²ˆ ì–´ë²„ì´ë‚  ì„ ë¬¼ì€ ë„ˆë¡œ ì •í–ˆë‹¤ğŸ\\\\ì§€ê¸ˆ ë¶€ëª¨ë‹˜ ì„ ë¬¼ êµ¬ë§¤í•˜ì‹œë©´ 2,OOOPë¥¼ ì‚¬ì€í’ˆìœ¼ë¡œ ë“œë¦½ë‹ˆë‹¤ğŸ’° ì„ ë¬¼ ì‚¬ê³  ì„ ë¬¼ ë°›ê³ ğŸµ ê¿© ë¨¹ê³  ì•Œ ë¨¹ê³ ğŸ’—",
        ],
        [
            "",
            "ST",
            "NF",
            "7ì¼ ë’¤ë©´ ì‚¬ë¼ì§€ëŠ” ì¿ í°...â—\\\\7/ì¼/ë’¤ í˜œíƒ ì¢…ë£Œ!! ì§€ê¸ˆ ì•±ì— ì ‘ì†í•´ì„œ ë¯¸ì‚¬ìš© ì¿ í° ì²´í¬âœ”ï¸ì²´í¬âœ”ï¸",
        ],
    ]

    for i in texts:
        temp = str(i[1]).replace("\\\\", "\n\n").replace("\\", "\n")
        print(f"ë°”ê¿€ ë¬¸êµ¬ : {temp}, ì‹œì¦Œ ì •ë³´ : {i[0]}")
        print(i[3])

        text = f"<season>{i[0]}<ctrl1>{i[1]}<ctrl2>{i[2]}{tokenizer.sep_token}{nlp.convert_to_other_unicode(i[3])}"

        encoded = tokenizer(text)
        sample = {k: torch.tensor([v]).to(device) for k, v in encoded.items()}
        with torch.no_grad():
            pred = model.generate(
                **sample,
                penalty_alpha=0.6,
                top_k=5,
                max_length=512,
                eos_token_id=tokenizer.eos_token_id,
            )
        print(f"ë°”ë€ ë¬¸êµ¬ : {i[2]}")
        # ì´ëª¨ì§€ë¡œ ì—­ì¹˜í™˜ - Predict í›„ì— ì ìš©í•´ì•¼ í•¨
        result = nlp.convert_emojis_in_text(
            nlp.convert_to_python_unicode(
                tokenizer.decode(pred[0], skip_special_tokens=True)
            )
        )
        print(result.replace("\\\\", "\n\n").replace("\\", "\n"))
        print("-" * 100)


if __name__ == "__main__":
    main()
